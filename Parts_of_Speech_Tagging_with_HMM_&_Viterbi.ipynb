{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parts_of_Speech_Tagging with_HMM_&_Viterbi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOC16rMVubnvSdo78+1swco",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shelviaandi/NLP_POS/blob/master/Parts_of_Speech_Tagging_with_HMM_%26_Viterbi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do6m2gfWVbi4",
        "colab_type": "text"
      },
      "source": [
        "## Parts-of-Speech Tagging (POS)\n",
        "\n",
        "The Notebook consists of :\n",
        "* Compute the transition matrix A in a Hidden Markov Model\n",
        "* Compute the transition matrix B in a Hidden Markov Model\n",
        "* Compute the Viterbi algorithm\n",
        "* Compute the model accuracy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-7wyaTwWUP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "\n",
        "# Punctuation characters\n",
        "punct = set(string.punctuation)\n",
        "\n",
        "# Morphology rules used to assign unknown word tokens\n",
        "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
        "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
        "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
        "adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
        "\n",
        "\n",
        "def get_word_tag(line, vocab): \n",
        "    if not line.split():\n",
        "        word = \"--n--\"\n",
        "        tag = \"--s--\"\n",
        "        return word, tag\n",
        "    else:\n",
        "        word, tag = line.split()\n",
        "        if word not in vocab: \n",
        "            # Handle unknown words\n",
        "            word = assign_unk(word)\n",
        "        return word, tag\n",
        "    return None \n",
        "\n",
        "\n",
        "def preprocess(vocab, data_fp):\n",
        "    \"\"\"\n",
        "    Preprocess data\n",
        "    \"\"\"\n",
        "    orig = []\n",
        "    prep = []\n",
        "\n",
        "    # Read data\n",
        "    with open(data_fp, \"r\") as data_file:\n",
        "\n",
        "        for cnt, word in enumerate(data_file):\n",
        "\n",
        "            # End of sentence\n",
        "            if not word.split():\n",
        "                orig.append(word.strip())\n",
        "                word = \"--n--\"\n",
        "                prep.append(word)\n",
        "                continue\n",
        "\n",
        "            # Handle unknown words\n",
        "            elif word.strip() not in vocab:\n",
        "                orig.append(word.strip())\n",
        "                word = assign_unk(word)\n",
        "                prep.append(word)\n",
        "                continue\n",
        "\n",
        "            else:\n",
        "                orig.append(word.strip())\n",
        "                prep.append(word.strip())\n",
        "\n",
        "    assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n",
        "    assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n",
        "\n",
        "    return orig, prep\n",
        "\n",
        "\n",
        "def assign_unk(tok):\n",
        "    \"\"\"\n",
        "    Assign unknown word tokens\n",
        "    \"\"\"\n",
        "    # Digits\n",
        "    if any(char.isdigit() for char in tok):\n",
        "        return \"--unk_digit--\"\n",
        "\n",
        "    # Punctuation\n",
        "    elif any(char in punct for char in tok):\n",
        "        return \"--unk_punct--\"\n",
        "\n",
        "    # Upper-case\n",
        "    elif any(char.isupper() for char in tok):\n",
        "        return \"--unk_upper--\"\n",
        "\n",
        "    # Nouns\n",
        "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
        "        return \"--unk_noun--\"\n",
        "\n",
        "    # Verbs\n",
        "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
        "        return \"--unk_verb--\"\n",
        "\n",
        "    # Adjectives\n",
        "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
        "        return \"--unk_adj--\"\n",
        "\n",
        "    # Adverbs\n",
        "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
        "        return \"--unk_adv--\"\n",
        "\n",
        "    return \"--unk--\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KI2YHixUyDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ3o3DhhXThg",
        "colab_type": "text"
      },
      "source": [
        "### Data Sources\n",
        "\n",
        "This notebook uses two tagged data sets collected from the Wall Street Journal (WSJ).\n",
        "\n",
        "[Here](http://relearn.be/2015/training-common-sense/sources/software/pattern-2.6-critical-fork/docs/html/mbsp-tags.html) is an example 'tag-set' or Part of Speech designation describing the two or three letter tag and their meaning.\n",
        "\n",
        "* One data set (WSJ-2_21.pos) will be used for training.\n",
        "  * The tagged training data has been preprocessed to form a vocabulary (hmm_vocab.txt)\n",
        "  * The words in the vocabulary are words from the training set that were used two or more times.\n",
        "  * The vocabulary is augmented with a set of 'unknown word tokens', described below.\n",
        "  * The training set will be used to create the emission, transmission and tag counts.\n",
        "\n",
        "* The other (WSJ-24.pos) for testing.\n",
        "  * The test set (WSJ-24.pos) is read in to create y\n",
        "  * This contains both the test text and the true tag.\n",
        "  * The test set has also been preprocessed to remove the tags to form test_words.txt.\n",
        "  * This is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions\n",
        "  * This forms the list prep, the preprocessed text used to test our POS taggers.\n",
        "  * A POS tagger will necessarily encounter words that are not in its datasets.\n",
        "  * To improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg8Da7nmVs-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "ab8e3333-e55e-4a99-9089-3557d316cd61"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhn2thljXnBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls \"/content/drive/My Drive/Colab Notebooks/Coursera/2_NLP_with_Probabilistic_Models/W2/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpHROKbVXuQ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d11acd40-f840-4b25-ef4e-185a8b5803dd"
      },
      "source": [
        "# Load in the training corpus\n",
        "wsj_02_21 = '/content/drive/My Drive/Colab Notebooks/Coursera/2_NLP_with_Probabilistic_Models/W2/viterbi_master/WSJ/WSJ_02-21.pos'\n",
        "with open(wsj_02_21, 'r') as f:\n",
        "    training_corpus = f.readlines()\n",
        "\n",
        "print(\"A few items of the training corpus list\")\n",
        "print(training_corpus[:5])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few items of the training corpus list\n",
            "['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plGe0UJsYfUP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "f78941fc-80b1-471a-d0e6-2ed62fb28462"
      },
      "source": [
        "# read the vocabulary data, split by each line of text and save the list\n",
        "hmm_vocab = '/content/drive/My Drive/Colab Notebooks/Coursera/2_NLP_with_Probabilistic_Models/W2/viterbi_master/data/hmm_vocab.txt'\n",
        "\n",
        "with open(hmm_vocab, 'r') as f:\n",
        "    voc_l = f.read().split('\\n')\n",
        "\n",
        "print(\"A few items of the vocabulary list\")\n",
        "print(voc_l[:50])\n",
        "print()\n",
        "print(\"A few items at the end of the vocabulary list\")\n",
        "print(voc_l[-50:])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few items of the vocabulary list\n",
            "['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\", \"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\", \"'til\", \"'ve\", '(', ')', ',', '-', '--', '--n--', '--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--', '.', '...', '0.01', '0.0108', '0.02', '0.03', '0.05', '0.1', '0.10', '0.12', '0.13', '0.15']\n",
            "\n",
            "A few items at the end of the vocabulary list\n",
            "['yards', 'yardstick', 'year', 'year-ago', 'year-before', 'year-earlier', 'year-end', 'year-on-year', 'year-round', 'year-to-date', 'year-to-year', 'yearlong', 'yearly', 'years', 'yeast', 'yelled', 'yelling', 'yellow', 'yen', 'yes', 'yesterday', 'yet', 'yield', 'yielded', 'yielding', 'yields', 'you', 'young', 'younger', 'youngest', 'youngsters', 'your', 'yourself', 'youth', 'youthful', 'yuppie', 'yuppies', 'zero', 'zero-coupon', 'zeroing', 'zeros', 'zinc', 'zip', 'zombie', 'zone', 'zones', 'zoning', '{', '}', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1ZGv4_-ZI9i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "a49867fa-63ad-4eab-89b7-80c7a757b9b8"
      },
      "source": [
        "#vocab : dictionary that has the index of the corresponding words\n",
        "vocab = {}\n",
        "\n",
        "#Get the index of the corresponding words\n",
        "for i, word in enumerate(sorted(voc_l)):\n",
        "  vocab[word] = i\n",
        "\n",
        "print(\"Vocabulary dictionary, key is the word, value is a unique integer\")\n",
        "cnt = 0\n",
        "for k, v in vocab.items():\n",
        "  print(f\"{k}:{v}\")\n",
        "  cnt += 1\n",
        "  if cnt > 20:\n",
        "    break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary dictionary, key is the word, value is a unique integer\n",
            ":0\n",
            "!:1\n",
            "#:2\n",
            "$:3\n",
            "%:4\n",
            "&:5\n",
            "':6\n",
            "'':7\n",
            "'40s:8\n",
            "'60s:9\n",
            "'70s:10\n",
            "'80s:11\n",
            "'86:12\n",
            "'90s:13\n",
            "'N:14\n",
            "'S:15\n",
            "'d:16\n",
            "'em:17\n",
            "'ll:18\n",
            "'m:19\n",
            "'n':20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_4IxQ6LZ3BV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7ca17c4b-dccb-4662-eca6-46fb78b7ab43"
      },
      "source": [
        "# Load in the test corpus\n",
        "wsj_24 = '/content/drive/My Drive/Colab Notebooks/Coursera/2_NLP_with_Probabilistic_Models/W2/viterbi_master/WSJ/WSJ_24.pos'\n",
        "with open(wsj_24, 'r') as f:\n",
        "  y = f.readlines()\n",
        "\n",
        "print(\"A sample of the test corpus\")\n",
        "print(y[:10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A sample of the test corpus\n",
            "['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6kaVT8BaR0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "495b8932-2781-4c7c-85c2-f9e741de306a"
      },
      "source": [
        "test_words = '/content/drive/My Drive/Colab Notebooks/Coursera/2_NLP_with_Probabilistic_Models/W2/test_words'\n",
        "\n",
        "# corpus without tags, preprocessed\n",
        "_, prep = preprocess(vocab, test_words)\n",
        "\n",
        "print(\"The length of the preprocessed test corpus: \", len(prep))\n",
        "print(\"This is a sample of the test_corpus: \")\n",
        "print(prep[:10])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of the preprocessed test corpus:  34198\n",
            "This is a sample of the test_corpus: \n",
            "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G0nRX4ycAsY",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: Parts-of-speech tagging\n",
        "\n",
        "### Part 1.1 - Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rUAFkfQazjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dictionaries(training_corpus, vocab):\n",
        "  \"\"\"\n",
        "  Input: \n",
        "        training_corpus: a corpus where each line has a word followed by its tag.\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "  Output: \n",
        "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
        "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
        "        tag_counts: a dictionary where the keys are the tags and the values are the counts\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  #Initialize the dictionaries using defaultdict\n",
        "  emission_counts = defaultdict(int)\n",
        "  transition_counts = defaultdict(int)\n",
        "  tag_counts = defaultdict(int)\n",
        "\n",
        "  # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n",
        "  prev_tag = '--s--'\n",
        "\n",
        "  # use 'i' to track the line number in the corpus\n",
        "  i = 0\n",
        "\n",
        "  # Each item in the training corpus contains a word and its POS tag\n",
        "  # Go through each word and its tag in the training corpus\n",
        "  for word_tag in training_corpus:\n",
        "\n",
        "    # Increment the word_tag count\n",
        "    i += 1\n",
        "\n",
        "    # Every 50,000 words, print the word count\n",
        "    if i % 50000 == 0:\n",
        "      print(\"word count = \", i)\n",
        "\n",
        "    # get the word and tag using the get_word_tag helper function \n",
        "    word, tag = get_word_tag(word_tag, vocab)\n",
        "\n",
        "    # Increment the transition count for the previous word and tag\n",
        "    transition_counts[(prev_tag, tag)] += 1\n",
        "\n",
        "    # Increment the emission count for the tag and word\n",
        "    emission_counts[(tag, word)] += 1\n",
        "\n",
        "    # Increment the tag count\n",
        "    tag_counts[tag] += 1\n",
        "\n",
        "    # Set the previous tag to this tag (for the next iteration of the loop)\n",
        "    prev_tag = tag\n",
        "\n",
        "  return emission_counts, transition_counts, tag_counts\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7p569HVe91X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "956c3479-380b-40e7-8462-e825039fb237"
      },
      "source": [
        "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word count =  50000\n",
            "word count =  100000\n",
            "word count =  150000\n",
            "word count =  200000\n",
            "word count =  250000\n",
            "word count =  300000\n",
            "word count =  350000\n",
            "word count =  400000\n",
            "word count =  450000\n",
            "word count =  500000\n",
            "word count =  550000\n",
            "word count =  600000\n",
            "word count =  650000\n",
            "word count =  700000\n",
            "word count =  750000\n",
            "word count =  800000\n",
            "word count =  850000\n",
            "word count =  900000\n",
            "word count =  950000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnP9rEjpfH9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "2a462412-1a34-4842-c4d8-658601908ab1"
      },
      "source": [
        "# get all the POS states\n",
        "states = sorted(tag_counts.keys())\n",
        "print(\"Number of POS tags ( number of 'states') : \", len(states))\n",
        "print(\"View these POS tags (states\")\n",
        "print(states)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of POS tags ( number of 'states') :  46\n",
            "View these POS tags (states\n",
            "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9_boDqjfeN_",
        "colab_type": "text"
      },
      "source": [
        "Notes : The 'states' are the Parts-of-speech designations found in the training data. They will also be referred to as 'tags' or POS.\n",
        "\n",
        "* \"NN\" is noun, singular,\n",
        "* 'NNS' is noun, plural.\n",
        "* In addition, there are helpful tags like '--s--' which indicate a start of a sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o65L7OMLfZFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "38c25eae-a355-4c79-8110-f67c868c4ddf"
      },
      "source": [
        "print(\"transition examples: \")\n",
        "for ex in list(transition_counts.items())[:3]:\n",
        "  print(ex)\n",
        "print()\n",
        "\n",
        "print(\"emission examples: \")\n",
        "for ex in list(emission_counts.items())[200:203]:\n",
        "  print(ex)\n",
        "print()\n",
        "\n",
        "print(\"ambiguous word examples: \")\n",
        "for tup, cnt in emission_counts.items():\n",
        "  if tup[1] == 'back': print(tup, cnt)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transition examples: \n",
            "(('--s--', 'IN'), 5050)\n",
            "(('IN', 'DT'), 32364)\n",
            "(('DT', 'NNP'), 9044)\n",
            "\n",
            "emission examples: \n",
            "(('DT', 'any'), 721)\n",
            "(('NN', 'decrease'), 7)\n",
            "(('NN', 'insider-trading'), 5)\n",
            "\n",
            "ambiguous word examples: \n",
            "('RB', 'back') 304\n",
            "('VB', 'back') 20\n",
            "('RP', 'back') 84\n",
            "('JJ', 'back') 25\n",
            "('NN', 'back') 29\n",
            "('VBP', 'back') 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cmvPa7bgcpn",
        "colab_type": "text"
      },
      "source": [
        "## Part 1.2 - Testing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfRI6fR4gR1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_pos(prep, y, emission_counts, vocab, states):\n",
        "    '''\n",
        "    Input: \n",
        "        prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
        "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
        "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "        states: a sorted list of all possible tags for this assignment\n",
        "    Output: \n",
        "        accuracy: Number of times you classified a word correctly\n",
        "    '''\n",
        "    # Initialize the number of correct predicitons to zero\n",
        "    num_correct = 0\n",
        "\n",
        "    # Get the (tag, word) tuples, sorted as a set\n",
        "    all_words = set(emission_counts.keys())\n",
        "    \n",
        "    # Get the number of (word, POS) tuples in the corpus 'y'\n",
        "    total = len(y)\n",
        "    for word, y_tup in zip(prep, y):\n",
        "\n",
        "      # Split the (word, POS) string into a list of 2 items\n",
        "      y_tup_l = y_tup.split()\n",
        "\n",
        "      # Verify that y_tup contain both word and POS\n",
        "      if len(y_tup_l) == 2:\n",
        "\n",
        "        #Set the true POS label for this word\n",
        "        true_label = y_tup_l[1]\n",
        "      \n",
        "      else:\n",
        "        #If the y_tup didnt contain word and POS, go to next word\n",
        "        continue\n",
        "      \n",
        "      count_final = 0\n",
        "      pos_final = ''\n",
        "\n",
        "      # If the word is in the vocabulary...\n",
        "      if word in vocab:\n",
        "        for pos in states:\n",
        "\n",
        "          #define the key as teh tuple containing the pos and word\n",
        "          key = (pos, word)\n",
        "\n",
        "          # check if the (pos, word) key exists in the emission_counts dictionary\n",
        "          if key in emission_counts:\n",
        "\n",
        "            #get the emission count of the (pos, word) tuple\n",
        "            count = emission_counts[key]\n",
        "\n",
        "            # keep track of the POS with the largest count\n",
        "            if count > count_final:\n",
        "\n",
        "              # Update the final count (largest count)\n",
        "              count_final = count\n",
        "\n",
        "              #Update the final POS\n",
        "              pos_final = pos\n",
        "\n",
        "        # If the final POS (with the largest count) mathes the true POS:\n",
        "        if pos_final == true_label:\n",
        "\n",
        "          # Update the number of correct predictions\n",
        "          num_correct += 1\n",
        "\n",
        "    accuracy = num_correct / total\n",
        "\n",
        "    return accuracy       "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVZwZjXBjZfE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63f5a186-dcf9-41d7-ea36-d2937bb89f01"
      },
      "source": [
        "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
        "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos: .4f} \" )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of prediction using predict_pos is  0.8889 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5CP8me-kG1U",
        "colab_type": "text"
      },
      "source": [
        "## Part 2.1 Generating Matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItlOWXlVOP3L",
        "colab_type": "text"
      },
      "source": [
        "### Create the `A` Transition Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xDRYqjzjnIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
        "    ''' \n",
        "    Input: \n",
        "        alpha: number used for smoothing\n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        transition_counts: transition count for the previous word and tag\n",
        "    Output:\n",
        "        A: matrix of dimension (num_tags,num_tags)\n",
        "    '''\n",
        "\n",
        "    # Get a sorted list of unique POS tags\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "\n",
        "    # Count the number of unique POS tags\n",
        "    num_tags = len(all_tags)\n",
        "\n",
        "    # Initialize the transition matrix 'A'\n",
        "    A = np.zeros((num_tags, num_tags))\n",
        "\n",
        "    # Get the unique transition tuples (previous POS, current POS)\n",
        "    trans_keys = set(transition_counts.keys())\n",
        "\n",
        "    # Go through each row of the transition matrix S\n",
        "    for i in range(num_tags):\n",
        "\n",
        "      # Go through each column of the transition matrix A\n",
        "      for j in range(num_tags):\n",
        "\n",
        "        # Initialize the count of the (prev POS, current POS) to zero\n",
        "        count = 0\n",
        "\n",
        "        # Define the tuple (prev POS, current POS)\n",
        "        # Get the tag at position i and tag at position j (from the all_tags list)a\n",
        "        key = (all_tags[i], all_tags[j])\n",
        "\n",
        "        #Check if the (prev POS, current POS) tuple exists in the transition counts dictionary\n",
        "        if key in transition_counts :\n",
        "\n",
        "          # Get count from the transition_counts dictionary for the (prev POS, current POS) tuple\n",
        "          count = transition_counts[key]\n",
        "\n",
        "        # Get the count of the previous tag (index position i) from tag counts\n",
        "        count_prev_tag = tag_counts[all_tags[i]]\n",
        "\n",
        "        # Apply smoothing using count of the tuple, alpha \n",
        "        # count of previous tag, alpha, and number of total tags\n",
        "        A[i, j] = (count + alpha) / (count_prev_tag + alpha * num_tags)\n",
        "\n",
        "    return A \n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpqS9ac4k6ZT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "2797559f-318c-41c1-9f95-686b002e558b"
      },
      "source": [
        "alpha = 0.001\n",
        "for i in range(46):\n",
        "    tag_counts.pop(i,None)\n",
        "\n",
        "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
        "# Testing your function\n",
        "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
        "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
        "\n",
        "#print(\"View a subset of transition matrix A\")\n",
        "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
        "print(A_sub)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A at row 0, col 0: 0.000007040\n",
            "A at row 3, col 1: 0.1691\n",
            "              RBS            RP           SYM        TO            UH\n",
            "RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\n",
            "RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\n",
            "SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\n",
            "TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\n",
            "UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVWq4QIXmmXL",
        "colab_type": "text"
      },
      "source": [
        "### Create the 'B' emission probabilities matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGmMeOpImKwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        alpha: tuning parameter used in smoothing \n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "    Output:\n",
        "        B: a matrix of dimension (num_tags, len(vocab))\n",
        "    '''\n",
        "    # Get the number of POS tag\n",
        "    num_tags = len(tag_counts)\n",
        "\n",
        "    # Get a list of all POS tags\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "\n",
        "    # Get the total number of unique words in the vocabulary\n",
        "    num_words = len(vocab)\n",
        "\n",
        "    # Initialize the emission matrix B with places for tags in the rows and words in the columns\n",
        "    B = np.zeros((num_tags, num_words))\n",
        "\n",
        "    # Get a set of all (POS, word) tuples from the keys of the emission_counts dictionary\n",
        "    emis_keys = set(list(emission_counts.keys()))\n",
        "\n",
        "    # Go through each row (POS tags)\n",
        "    for i in range(num_tags):\n",
        "\n",
        "      # Go through each column (words)\n",
        "      for j in range(num_words):\n",
        "\n",
        "          # Initialize the emission count fof the (POS tag, word) to zero\n",
        "          count = 0\n",
        "\n",
        "          # Define the (POS tag, word) tuple for this row and column\n",
        "          key = (all_tags[i], vocab[j])\n",
        "\n",
        "      # check if the (POS tag, word) tuple exists as a key in emission counts\n",
        "          if key in emission_counts.keys():\n",
        "              # Get the count of (POS tag, word) from the emission_counts d\n",
        "              count = emission_counts[key]\n",
        "\n",
        "          # Get the count of the POS tag\n",
        "          count_tag = tag_counts[all_tags[i]]\n",
        "\n",
        "          # Apply smoothing and store the smoothed value into the emission matrix B for this row and column\n",
        "          B[i, j] = (count + alpha) / (count_tag + alpha * num_words)\n",
        "    \n",
        "    return B"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2hg3zfEqABM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "88bd72c1-f133-448c-b44b-4c81b3fe14e5"
      },
      "source": [
        "# creating your emission probability matrix. this takes a few minutes to run. \n",
        "for i in range(46):\n",
        "    tag_counts.pop(i,None)\n",
        "\n",
        "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
        "\n",
        "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
        "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
        "\n",
        "# Try viewing emissions for a few words in a sample dataframe\n",
        "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
        "\n",
        "# Get the integer ID for each word\n",
        "cols = [vocab[a] for a in cidx]\n",
        "\n",
        "# Choose POS tags to show in a sample dataframe\n",
        "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
        "\n",
        "# For each POS tag, get the row number from the 'states' list\n",
        "rows = [states.index(a) for a in rvals]\n",
        "\n",
        "# Get the emissions for the sample of words, and the sample of POS tags\n",
        "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
        "print(B_sub)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "View Matrix position at row 0, column 0: 0.000006032\n",
            "View Matrix position at row 3, column 1: 0.000000720\n",
            "              725      adroitly     engineers      promoted       synergy\n",
            "CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\n",
            "NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
            "NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
            "VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
            "RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
            "RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHDT-52irTHK",
        "colab_type": "text"
      },
      "source": [
        "## Part 3: Viterbi Algorithm and Dynamic Programming\n",
        "\n",
        "\n",
        "* Initialization - Initializing the best_paths and best_probabilities matrices that will be populating in feed_forward.\n",
        "* Feed forward - At each step, calculating the probability of each path happening and the best paths up to that point.\n",
        "* Feed backward: Finding the best path with the highest probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9-pSCrb-WsH",
        "colab_type": "text"
      },
      "source": [
        "### Part 3.1: Initialization\n",
        "You will start by initializing two matrices of the same dimension.\n",
        "\n",
        "best_probs: Each cell contains the probability of going from one POS tag to a word in the corpus.\n",
        "\n",
        "best_paths: A matrix that helps you trace through the best possible path in the corpus.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYfeh1e5rNZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        states: a list of all possible parts-of-speech\n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
        "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
        "        corpus: a sequence of words whose POS is to be identified in a list \n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "    Output:\n",
        "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
        "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
        "    '''\n",
        "\n",
        "    # Get the total number of unique POS tags\n",
        "    num_tags = len(tag_counts)\n",
        "\n",
        "    # Initialize best_probs matrix\n",
        "    # POS tags in the rows, number of words in the corpus as the columns\n",
        "    best_probs = np.zeros((num_tags, len(corpus)))\n",
        "\n",
        "    # Initialize best_paths matrix\n",
        "    # POS tags in the rows, number of words in the corpus as columns\n",
        "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
        "\n",
        "    # Define the start token\n",
        "    s_idx = states.index(\"--s--\")\n",
        "\n",
        "    # Go through each of the POS tags\n",
        "    for i in range(num_tags):\n",
        "\n",
        "      # Handle the special case when the transition from start token to POS tag i is zero\n",
        "      if A[s_idx, i] == 0:\n",
        "\n",
        "        # Initialize best_probs at POS tag 'i', column 0, to negative infinity\n",
        "        best_probs[i, 0] = float('-inf')\n",
        "\n",
        "      # For all other cases when transition from start token to POS tag i is non-zero:\n",
        "      else:\n",
        "\n",
        "        # Initialize best_probs at POS tag 'i', column 0\n",
        "        # Check the formula in the instructions above\n",
        "        best_probs[i, 0] = math.log(A[s_idx, i]) + math.log(B[i, vocab[corpus[0]]])\n",
        "\n",
        "    return best_probs, best_paths      "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27vd-L76AKM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCoqlnRXAMY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3ce498a7-cf84-4f62-c1a5-513c7a3fa520"
      },
      "source": [
        "# Test the function\n",
        "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\")\n",
        "print(f\"best_probs[2,3]: {best_probs[2,3]:.4f}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best_probs[0,0]: -22.6098\n",
            "best_probs[2,3]: 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBz03J-EBLUK",
        "colab_type": "text"
      },
      "source": [
        "### Part 3.2 Viterbi Forward\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I93ANslUAf9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        A, B: The transiton and emission matrices respectively\n",
        "        test_corpus: a list containing a preprocessed corpus\n",
        "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
        "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
        "    Output: \n",
        "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
        "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
        "    '''\n",
        "\n",
        "    # Get the number of unique POS tags (which is the num of rows in best_probs)\n",
        "    num_tags = best_probs.shape[0]\n",
        "\n",
        "    # Go through every word in corpus starting from word 1\n",
        "    # Recall that word 0 was initialized in `initialize`\n",
        "    for i in range(1, len(test_corpus)):\n",
        "\n",
        "      # Print number of words processed, every 5000 words\n",
        "      if i % 5000 == 0:\n",
        "        print(\"Word processed: {:>8}\".format(i))\n",
        "\n",
        "      for j in range(num_tags):\n",
        "\n",
        "        # initialize best_prob for word i to negative infinity\n",
        "        best_prob_i = float(\"-inf\")\n",
        "\n",
        "        # Initialize best_path for current word i to None\n",
        "        best_path_i = None\n",
        "\n",
        "        # For each POS tag that the previous word can be:\n",
        "        for k in range(num_tags):\n",
        "          # Calculate the probability = \n",
        "          # best probs of POS tag k, previous word i-1 + \n",
        "          # log(prob of transition from POS k to POS j) + \n",
        "          # log(prob that emission of POS j is word i)\n",
        "          prob = best_probs[k, i-1] + math.log(A[k, j]) + math.log(B[j, vocab[test_corpus[i]]])\n",
        "          \n",
        "          # check if this path's probability is greater than\n",
        "          # the best probability up to and before this point\n",
        "          if prob > best_prob_i:\n",
        "            # keep track of the best probability\n",
        "            best_prob_i = prob\n",
        "\n",
        "            # keep track of the POS tag of the previous word\n",
        "            # that is part of the best path.  \n",
        "            # Save the index (integer) associated with \n",
        "            # that previous word's POS tag\n",
        "            best_path_i = k\n",
        "\n",
        "        # Save the best probability for the \n",
        "        # given current word's POS tag\n",
        "        # and the position of the current word inside the corpus\n",
        "        best_probs[j,i] = best_prob_i\n",
        "        # Save the unique integer ID of the previous POS tag\n",
        "        # into best_paths matrix, for the POS tag of the current word\n",
        "        # and the position of the current word inside the corpus.\n",
        "        best_paths[j,i] = best_path_i\n",
        "\n",
        "    return best_probs, best_paths    "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUWGXdNKDXg7",
        "colab_type": "text"
      },
      "source": [
        "Run the viterbi_forward function to fill in the best_probs and best_paths matrices.\n",
        "\n",
        "Note that this will take a few minutes to run. There are about 30,000 words to process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ1AbWYyDVWd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5b7a31a0-15ad-4dbf-efce-c5dcee8c1494"
      },
      "source": [
        "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word processed:     5000\n",
            "Word processed:    10000\n",
            "Word processed:    15000\n",
            "Word processed:    20000\n",
            "Word processed:    25000\n",
            "Word processed:    30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H8_xh5SDhW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6b1f9e44-e86c-4086-d01f-2e330d46b6ce"
      },
      "source": [
        "# Test this function \n",
        "print(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \n",
        "print(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best_probs[0,1]: -24.7822\n",
            "best_probs[0,4]: -49.5601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG7Gk6tNEhd9",
        "colab_type": "text"
      },
      "source": [
        "### Part 3.3 Viterbi backward\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eisBC_8HELuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
        "  '''\n",
        "    This function returns the best path.\n",
        "    \n",
        "  '''\n",
        "  # Get the number of words in the corpus\n",
        "  # which is also the number of columns in best_probs, best_paths\n",
        "  m = best_paths.shape[1]\n",
        "\n",
        "  # Initialize array z, same length as the corpus\n",
        "  z = [None] * m\n",
        "\n",
        "  # Get the number of unique POS tags\n",
        "  num_tags = best_probs.shape[0]\n",
        "\n",
        "  # Initialize the best probability for the last word\n",
        "  best_prob_for_last_word = float('-inf')\n",
        "\n",
        "  # Initialize pred array, same length as corpus\n",
        "  pred = [None] * m\n",
        "\n",
        "  # Go through each POS tag for the last word (last column of best_probs)\n",
        "  # in order to find the row (POS tag integer ID) \n",
        "  # with highest probability for the last word\n",
        "  for k in range(num_tags):\n",
        "\n",
        "    # If the probability of POS tag at row k \n",
        "    # is better than the previosly best probability for the last word:\n",
        "    if best_probs[k, m-1] > best_prob_for_last_word:\n",
        "\n",
        "      # Store the new best probability for the last word\n",
        "      best_prob_for_last_word = best_probs[k, m-1]\n",
        "\n",
        "      # Store the unique integer ID of the POS tag\n",
        "      # which is also the row number in best_probs\n",
        "      z[m-1] = k\n",
        "\n",
        "  # Convert the last word's predicted POS tag\n",
        "  # from its unique integer ID into the string representation\n",
        "  # using the 'states' dictionary\n",
        "  # store this in the 'pred' array for the last word\n",
        "  pred[m-1] = states[z[m - 1]]\n",
        "\n",
        "  ## Step 2 ##\n",
        "  # Find the best POS tags by walking backward through the best_paths\n",
        "  # From the last word in the corpus to the 0th word in the corpus\n",
        "  for i in range(m-1, -1, -1):\n",
        "\n",
        "    # Retrieve the unique integer ID of\n",
        "    # the POS tag for the word at position 'i' in the corpus\n",
        "    pos_tag_for_word_i = z[i]\n",
        "\n",
        "    # In best_paths, go to the row representing the POS tag of word i\n",
        "    # and the column representing the word's position in the corpus\n",
        "    # to retrieve the predicted POS for the word at position i-1 in the corpus\n",
        "    z[i - 1] = best_paths[pos_tag_for_word_i, i]\n",
        "\n",
        "    # Get the previous word's POS tag in string form\n",
        "    # Use the 'states' dictionary, \n",
        "    # where the key is the unique integer ID of the POS tag,\n",
        "    # and the value is the string representation of that POS tag\n",
        "    pred[i - 1] = states[z[i - 1]]\n",
        "\n",
        "  return pred"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XRSvoGyGoAE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "a75638df-0c10-419b-f9d4-7c372b323b9e"
      },
      "source": [
        "\n",
        "# Run and test your function\n",
        "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
        "m=len(pred)\n",
        "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
        "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The prediction for pred[-7:m-1] is: \n",
            " ['not', 'see', 'them', 'here', 'with', 'us'] \n",
            " ['RB', 'VB', 'PRP', 'RB', 'IN', 'PRP'] \n",
            "\n",
            "The prediction for pred[0:8] is: \n",
            " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN'] \n",
            " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk5vMXUsIx5F",
        "colab_type": "text"
      },
      "source": [
        "## Part 4: Predicting on a data set\n",
        "Compute the accuracy of your prediction by comparing it with the true y labels.\n",
        "\n",
        "* pred is a list of predicted POS tags corresponding to the words of the test_corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwj2ooU7GrGz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a0169dc4-05e7-4f86-d4e8-2e7b61e1f89b"
      },
      "source": [
        "print('The third word is:', prep[3])\n",
        "print('Your prediction is:', pred[3])\n",
        "print('Your corresponding label y is: ', y[3])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The third word is: temperature\n",
            "Your prediction is: NN\n",
            "Your corresponding label y is:  temperature\tNN\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lauwvrEKI6ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(pred, y):\n",
        "    '''\n",
        "    Input: \n",
        "        pred: a list of the predicted parts-of-speech \n",
        "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
        "    Output: \n",
        "        \n",
        "    '''\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Zip together the prediction and the labels\n",
        "    for prediction, y in zip(pred, y):\n",
        "\n",
        "      # Split the label into the word and the POS tag\n",
        "      word_tag_tuple = y.split()\n",
        "\n",
        "      # Check that there is actually a word and a tag\n",
        "      # no more and no less than 2 items\n",
        "      if len(word_tag_tuple) != 2:\n",
        "        continue\n",
        "\n",
        "      # store the word and tag separately\n",
        "      word, tag  = word_tag_tuple\n",
        "\n",
        "      # Check if the POS tag label matches the prediction\n",
        "      if prediction == tag:\n",
        "\n",
        "        # count the number of times that the prediction and label match\n",
        "        num_correct += 1\n",
        "\n",
        "      # keep track of the total number of examples (that have valid labels)\n",
        "      total += 1\n",
        "\n",
        "    return(num_correct/total)  "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCToUZ70J-26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4df89045-984a-4797-bce3-1bd613b81b0d"
      },
      "source": [
        "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the Viterbi algorithm is 0.9530\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya_nMtB0KArk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}